meta:
    software_dirs:
        voyager: "/u/cmolder/GitHub/voyager/"
        champsim: "/u/cmolder/GitHub/ChampSim/"
    trace_dirs:
        load: "/scratch/cluster/qduong/ML-DPC/data/load_traces/"
        champsim: "/scratch/cluster/cmolder/ML-DPC/data/champsim_traces/"
    sweep_dir: "/scratch/cluster/cmolder/voyager_hypertuning/lstm/"
    use_gpu: true
    print_every: 100
    checkpoint_every: 10000
    champsim_print_every: 1 # million instructions 
traces: ["spec17/605.mcf-s0.txt.xz"]
config: # If an array, it generates permutations of the values. Otherwise, it keeps them constant.
    name: "Base Voyager"
    # Training Parameters
    batch_size: 512
    learning_rate: 0.0001
    learning_rate_decay: 2
    min_learning_rate: 0.0001
    num_epochs: 25
    num_epochs_online: 20
    steps_per_epoch: 80000
    train_split: 0.8
    valid_split: 0.1
    multi_label: False
    # Input Parameters
    sequence_loss: False    # Train with sequence loss instead of cross entropy on last timestep
    use_current_pc: False   # Feed in PC of current load instead of previous load at each timestep
    pc_localization: False  # Train on PC localized stream instead of global stream
    sequence_length: 16
    offset_bits: 6
    # LSTM Parameters
    pc_embed_size: 64
    page_embed_size: 64
    num_experts: 100
    lstm_dropout: [0.6, 0.8]
    lstm_size: [32, 64, 128, 256]
    lstm_layers: [1, 2]
